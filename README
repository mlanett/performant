Records four fundamental performance metrics:

The observation interval under observation.
The number of operations in the interval.
The total time during which operations resided in the system — the “busy time.”
The total execution time of all operations — the “weighted time.”

What is this good for? We can use these together with Little’s Law to derive these additional four long-term average metrics:

Throughput: divide the number of queries in the observation interval by the length of the interval.
Execution time: divide the weighted time by the number of queries in the interval.
Concurrency: divide the weighted time by the length of the interval.
Utilization: divide the busy time by the length of the interval.

How do we calculate this?

The approach which works is to have each job execute two actions:
  Start:
    Increment the basic metrics.
    Add itself to a running job set, with a timeout.
  Finish:
    Check if it is still in the running job set.
    If so,
      Remove itself from the running job set.
      Increment/decrement the basic metrics.

One problem with this approach is that start and finish actions must be balanced, but jobs can die.
So relying on the job to execute the finish action is not robust.
This is solved by the monitor.

At the same time we need to processes to run continuously.
  Sampler:
    The sampler reads the basic metrics and calculates the long-term average metrics.
  Monitor:
    The monitor checks for jobs which died without executing Finish actions.
    It removes them from the running job set and performs the Finish action for them.

Redis Keys:

Performant:Kind:Running     < SortedSet < ID scored by Expire time > >
Performant:Kind:Operations  < integer > = Size of {Running}
Performant:Kind:Busy Time   < float >
Performant:Kind:Work Time   < float >
Performant:Kind:Last Tick   < float >

Intervals for rollups:
  1 second for at least the past 1 minute (60...120 records)
  1 minute for at least the past hour (60..120 records)
  hourly for at least the past 3 days (72...144 records)
  daily after that
  

See also:
http://www.mysqlperformanceblog.com/2011/04/27/the-four-fundamental-performance-metrics/
http://www.mysqlperformanceblog.com/2011/05/05/the-two-even-more-fundamental-performance-metrics/

Sampler daemon ideas:
The sampler may/will need a specific configuration file.
It will run from an installed gem.
e.g. sampler -c ~/foo/bar/config.yml -e production COMMAND
Pid and Log file location may be set in the config file.
It might be better if the configuration file is self-executing, e.g.
  ~/foo/bar/sampler -e production COMMAND
It will need to support bundler, e.g.
  ( cd ~/foo/bar; bundle exec sampler -e production COMMAND )
